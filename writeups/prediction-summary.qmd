---
title: "Predictive modeling of claims status"
author: 'YOUR NAMES HERE'
date: today
---

### Abstract

Provide a 3-5 sentence summary of your work on the primary task. Indicate what input data was used, what method was used for binary class predictions, what method was used for multiclass predictions, and what estimated accuracies were achieved.

> *Header and paragraph content was scraped from the raw webpages and processed into term frequencies of word tokens. For binary classification, a two-layer neural network yielded an estimated 81.4% accuracy; for multiclass classification, a support vector machine gave 78% accuracy.*

### Preprocessing

In one paragraph lay out your preprocessing pipeline. No need to provide exact step-by-step detail; just give an overview of the main components:

-   what text content was extracted from HTML

-   how text was cleaned

-   how cleaned text was represented quantitatively

The preprocessing pipeline began by extracting the main textual content from the HTML claim files, including paragraphs, headings, and list items, while dropping scripts, style information, menus, and other non-textual elements. After extraction, the text was cleaned by removing punctuation, converting to lowercase, stripping HTML artifacts, and normalizing whitespace so that each document produced a consistent and readable “text_clean” field. The cleaned text was then converted into a quantitative representation using the Keras text vectorization layer. This layer tokenized the text, restricted the vocabulary to the twenty thousand most frequent tokens, and converted each document into a fixed-length sequence of two hundred integer token IDs. These token sequences served as the numerical inputs for both the binary and multiclass predictive models.

### Methods

Describe your final predictive models. Include one paragraph with details on the binary classification approach, and one on the multiclass approach. Include for each:

-   what ML/statistical method was used

-   model specification and hyperparameter selection

-   training method

The binary prediction model was implemented using a neural sequence architecture created with Keras and TensorFlow. The model began with a text vectorization layer that transformed the raw text into fixed-length token sequences. These were passed into an embedding layer with sixty-four dimensions and masking enabled, followed by a bidirectional LSTM with thirty-two units so that the model could capture context from both directions of the text. A dense layer with thirty-two ReLU units provided additional nonlinear transformation, and a final single-unit sigmoid output was used to model the probability of belonging to class one. Hyperparameters such as sequence length, vocabulary size, embedding dimension, and LSTM size were chosen using standard practices for medium-sized NLP problems. The training procedure used the Adam optimizer, binary cross-entropy loss, a batch size of thirty-two, and five training epochs with a twenty percent validation split.

The multiclass model used the same general architecture as the binary model but with adjustments for five output categories. Text was vectorized into integer token sequences using the same vocabulary size and sequence length settings. These sequences were fed into a sixty-four-dimension embedding layer and then through a bidirectional LSTM with thirty-two units. A dense layer with thirty-two ReLU units produced intermediate representations prior to the output layer. The final layer used a softmax activation with five units so that the model output a probability distribution over the five classes. Hyperparameters were kept consistent with the binary model for comparability. The model was trained using the Adam optimizer, sparse categorical cross-entropy loss, a batch size of thirty-two, eight training epochs, and a twenty percent validation split. Class predictions were obtained by selecting the class with the highest predicted probability.

### Results

Indicate the predictive accuracy of the binary classifications and the multiclass classifications. Provide a table for each, and report sensitivity, specificity, and accuracy.[^1]

```{r}
combined_results <- read_csv("results/combined_metrics.csv")
combined_results
```

[^1]: Read [this article](https://yardstick.tidymodels.org/articles/multiclass.html) on multiclass averaging.
